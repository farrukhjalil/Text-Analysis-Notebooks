{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1>Using the Mann-Whitney U Test</h1>\n",
    "USES PYTHON 3.x\n",
    "\n",
    "This notebook will run through how to do the <a href=\"https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test\">Mann-Whitney U test</a>, comparing the word usage across two corpora.\n",
    "\n",
    "First we will specify all the directories and filenames that will be used in our code. Keeping them up here makes them easier to find and change later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CORPUS1_PATH = \"corpus1/\" # The directory to look in for all the text files to be analyzed.\n",
    "CORPUS2_PATH = \"corpus2/\" # The directory to look in for all the text files to be analyzed.\n",
    "\n",
    "#The names (and locations) of the csv files that will be created:\n",
    "WORD_FREQUENCY_CSV_FILENAME1 = \"Corpus1Frequencies.csv\"\n",
    "WORD_FREQUENCY_CSV_FILENAME2 = \"Corpus2Frequencies.csv\"\n",
    "CORPUS_COMPARISON_FILENAME = \"CorpusCompare.csv\"\n",
    "WORD_LIST_FILENAME = \"words.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Find All the Files to be Analyzed</h1>\n",
    "Same as with the TF-IDF code, this next section is some very simple code that finds all the text files, and then keeps track of where they are located, so that future functions can find them all when they need to do processing on them. This is cheaper than trying to keep the contents of all the text files in memory. This is a good starting point for any sort of text analysis.\n",
    "\n",
    "One difference from the TF-IDF code is that text files from two different corpora are being tracked, as opposed to text files from just one corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "corpus1dirs = os.listdir(CORPUS1_PATH) # returns list\n",
    "corpus2dirs = os.listdir(CORPUS2_PATH) # returns list\n",
    "corpus1 = []\n",
    "corpus2 = []\n",
    "\n",
    "#Loop over all of the files in the provided directory\n",
    "for file in corpus1dirs:\n",
    "    #Ensure that only text files are included:\n",
    "    if file.endswith(\".txt\"):\n",
    "        text_dir = os.path.join(CORPUS1_PATH, file)\n",
    "        corpus1.append(text_dir)\n",
    "\n",
    "#Loop over all of the files in the provided directory\n",
    "for file in corpus2dirs:\n",
    "    #Ensure that only text files are included:\n",
    "    if file.endswith(\".txt\"):\n",
    "        text_dir = os.path.join(CORPUS2_PATH, file)\n",
    "        corpus2.append(text_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Term Frequency</h1>\n",
    "\n",
    "The Mann-Whitney U test needs some measure to rank by, so the the code below generates Corpus1Frequencies.csv and Corpus2Frequencies.csv files containing the relative frequencies for each word in each of the files in both corpora.\n",
    "\n",
    "<b>NOTE:</b> Other measures could be used here (such as raw frequency, for example) and may be better suited to what you're trying to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done first corpus.\n",
      "Done second corpus.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "word_list = []\n",
    "\n",
    "def findFreq(corpus, csvFile):\n",
    "\n",
    "    texts = []\n",
    "    docs = {}\n",
    "    num_words = 0\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    for text in corpus:\n",
    "        num_words = 0\n",
    "        with open(text, 'r', encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                # Use Regex to remove punctuation and isolate words\n",
    "                words = re.findall(r'(\\b\\S+\\b|#\\w+|@\\w+)', line.lower())\n",
    "                for word in words:\n",
    "                    counts[word] += 1\n",
    "                num_words += len(words)\n",
    "\n",
    "        relativefreqs = {}\n",
    "        for word, rawCount in counts.items():\n",
    "            word_list.append(word)\n",
    "            relativefreqs[word] = rawCount / float(num_words)\n",
    "            counts[word] = 0\n",
    "        # add this document's relative freqs to our dictionary\n",
    "        docs[os.path.basename(text)] = relativefreqs\n",
    "        #print(\"Done with \" + text)\n",
    "\n",
    "    #output everything to a .csv file, using pandas as a go between.\n",
    "    df = pd.DataFrame(docs)\n",
    "    df = df.fillna(0)\n",
    "    df.to_csv(csvFile, encoding=\"utf-8\") # write out to CSV\n",
    "\n",
    "findFreq(corpus1, WORD_FREQUENCY_CSV_FILENAME1)\n",
    "print(\"Done first corpus.\")\n",
    "\n",
    "findFreq(corpus2, WORD_FREQUENCY_CSV_FILENAME2)\n",
    "print(\"Done second corpus.\")\n",
    "\n",
    "target = open(WORD_LIST_FILENAME, 'w', encoding=\"utf-8\")\n",
    "\n",
    "unique_words = set(word_list)\n",
    "for word in sorted(unique_words):\n",
    "    target.write(str(word) + \"\\n\")\n",
    "target.close()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Mann–Whitney U Test</h1>\n",
    "The code below then iterates over both of the .csvs generated above, comparing each word across both corpora. If there is a word that only appears in one of the corpora, then a row of zeroes is generated for the corpus that does not contain that word.\n",
    "\n",
    "A .csv file is generated, which shows the ranking for every word, according to whether it is more salient to corpus 2 versus corpus 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "df1 = pd.read_csv(WORD_FREQUENCY_CSV_FILENAME1, index_col=0) # read in the CSV\n",
    "df1.rename(columns={'Unnamed: 0': 'Text'}, inplace=True) # add a label to the first column\n",
    "df1 = df1.fillna(0) # replace NaNs with zeroes.\n",
    "\n",
    "df2 = pd.read_csv(WORD_FREQUENCY_CSV_FILENAME2, index_col=0) # read in the CSV\n",
    "df2.rename(columns={'Unnamed: 0': 'Text'}, inplace=True) # add a label to the first column\n",
    "df2 = df2.fillna(0) # replace NaNs with zeroes.\n",
    "\n",
    "total_docs = len(df1.columns) * len(df2.columns)\n",
    "\n",
    "# Make \"dummy\" rows of all zeroes for any words that only appear in one corpus and not the other\n",
    "missingInCorpus1 = []\n",
    "missingInCorpus2 = []\n",
    "for i in range(0, df1.shape[1]):\n",
    "    missingInCorpus1.append(0)    \n",
    "\n",
    "for i in range(0, df2.shape[1]):\n",
    "    missingInCorpus2.append(0)\n",
    "\n",
    "# Iterate over the wordlist and the two corpora, and output to csv\n",
    "with open(CORPUS_COMPARISON_FILENAME, 'w', newline='', encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    writer.writerow(['word', 'Mann Whitney U Value', 'Mann Whitney rho-value'])\n",
    "    with open(WORD_LIST_FILENAME, 'r', encoding=\"utf-8\") as f:\n",
    "        for word in f:\n",
    "            word = word.strip()\n",
    "            if (word in df1.index):\n",
    "                countsInCorpus1 = df1.loc[word].values\n",
    "            else:\n",
    "                countsInCorpus1 = missingInCorpus1\n",
    "            if (word in df2.index):\n",
    "                countsInCorpus2 = df2.loc[word].values\n",
    "            else:\n",
    "                countsInCorpus2 = missingInCorpus2\n",
    "            try:\n",
    "                mw = mannwhitneyu(countsInCorpus1, countsInCorpus2)\n",
    "                mwStat = mw.statistic\n",
    "                mwRho = mwStat / total_docs\n",
    "            except ValueError: # Was having problems with this earlier, so this is mainly for debugging reasons\n",
    "                mwStat = -1\n",
    "                mwRho = -1\n",
    "            writer.writerow([word, mwStat, mwRho])\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tables and Graphs</h1>\n",
    "\n",
    "Let's look at a table showing the highest ranked words (likely to be ones that are more salient to corpus 1). Sorted by the Mann-Whitney ρ value, which is just the U rank divided by the total number of documents in each corpus, multiplied together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mann Whitney U Value</th>\n",
       "      <th>Mann Whitney rho-value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sit</th>\n",
       "      <td>147.0</td>\n",
       "      <td>0.918750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vexed</th>\n",
       "      <td>146.0</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>curls</th>\n",
       "      <td>146.0</td>\n",
       "      <td>0.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cheerful</th>\n",
       "      <td>145.0</td>\n",
       "      <td>0.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usually</th>\n",
       "      <td>143.0</td>\n",
       "      <td>0.893750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roused</th>\n",
       "      <td>141.0</td>\n",
       "      <td>0.881250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forgetting</th>\n",
       "      <td>141.0</td>\n",
       "      <td>0.881250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cousin</th>\n",
       "      <td>140.0</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impatient</th>\n",
       "      <td>140.0</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tying</th>\n",
       "      <td>140.0</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lingered</th>\n",
       "      <td>139.0</td>\n",
       "      <td>0.868750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cordial</th>\n",
       "      <td>139.0</td>\n",
       "      <td>0.868750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vex</th>\n",
       "      <td>138.0</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>138.0</td>\n",
       "      <td>0.862500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pinched</th>\n",
       "      <td>137.5</td>\n",
       "      <td>0.859375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fields</th>\n",
       "      <td>137.0</td>\n",
       "      <td>0.856250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rug</th>\n",
       "      <td>135.5</td>\n",
       "      <td>0.846875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gather</th>\n",
       "      <td>135.0</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stroked</th>\n",
       "      <td>135.0</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doubtfully</th>\n",
       "      <td>135.0</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>impatiently</th>\n",
       "      <td>135.0</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arabian</th>\n",
       "      <td>135.0</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blotted</th>\n",
       "      <td>134.5</td>\n",
       "      <td>0.840625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>errand</th>\n",
       "      <td>133.0</td>\n",
       "      <td>0.831250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>every-day</th>\n",
       "      <td>133.0</td>\n",
       "      <td>0.831250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>haste</th>\n",
       "      <td>133.0</td>\n",
       "      <td>0.831250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waking</th>\n",
       "      <td>132.0</td>\n",
       "      <td>0.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>drying</th>\n",
       "      <td>132.0</td>\n",
       "      <td>0.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cellar</th>\n",
       "      <td>132.0</td>\n",
       "      <td>0.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laugh</th>\n",
       "      <td>132.0</td>\n",
       "      <td>0.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lifted</th>\n",
       "      <td>127.0</td>\n",
       "      <td>0.793750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unused</th>\n",
       "      <td>127.0</td>\n",
       "      <td>0.793750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concluded</th>\n",
       "      <td>127.0</td>\n",
       "      <td>0.793750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tone</th>\n",
       "      <td>127.0</td>\n",
       "      <td>0.793750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sew</th>\n",
       "      <td>127.0</td>\n",
       "      <td>0.793750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>folds</th>\n",
       "      <td>127.0</td>\n",
       "      <td>0.793750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grave</th>\n",
       "      <td>127.0</td>\n",
       "      <td>0.793750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eagerness</th>\n",
       "      <td>127.0</td>\n",
       "      <td>0.793750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sunday</th>\n",
       "      <td>127.0</td>\n",
       "      <td>0.793750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>milder</th>\n",
       "      <td>127.0</td>\n",
       "      <td>0.793750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wonder</th>\n",
       "      <td>127.0</td>\n",
       "      <td>0.793750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wishing</th>\n",
       "      <td>126.5</td>\n",
       "      <td>0.790625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointed</th>\n",
       "      <td>126.0</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knitting</th>\n",
       "      <td>126.0</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slip</th>\n",
       "      <td>126.0</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>never</th>\n",
       "      <td>126.0</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>foreigners</th>\n",
       "      <td>126.0</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>married</th>\n",
       "      <td>126.0</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fold</th>\n",
       "      <td>126.0</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>winter</th>\n",
       "      <td>126.0</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ignorant</th>\n",
       "      <td>126.0</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banish</th>\n",
       "      <td>126.0</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>burden</th>\n",
       "      <td>125.5</td>\n",
       "      <td>0.784375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pitied</th>\n",
       "      <td>125.5</td>\n",
       "      <td>0.784375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>helped</th>\n",
       "      <td>125.0</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looked</th>\n",
       "      <td>125.0</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stupid</th>\n",
       "      <td>125.0</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>craving</th>\n",
       "      <td>125.0</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>blooming</th>\n",
       "      <td>125.0</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>burnt</th>\n",
       "      <td>125.0</td>\n",
       "      <td>0.781250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Mann Whitney U Value  Mann Whitney rho-value\n",
       "word                                                      \n",
       "sit                          147.0                0.918750\n",
       "vexed                        146.0                0.912500\n",
       "curls                        146.0                0.912500\n",
       "cheerful                     145.0                0.906250\n",
       "usually                      143.0                0.893750\n",
       "roused                       141.0                0.881250\n",
       "forgetting                   141.0                0.881250\n",
       "cousin                       140.0                0.875000\n",
       "impatient                    140.0                0.875000\n",
       "tying                        140.0                0.875000\n",
       "lingered                     139.0                0.868750\n",
       "cordial                      139.0                0.868750\n",
       "vex                          138.0                0.862500\n",
       "used                         138.0                0.862500\n",
       "pinched                      137.5                0.859375\n",
       "fields                       137.0                0.856250\n",
       "rug                          135.5                0.846875\n",
       "gather                       135.0                0.843750\n",
       "stroked                      135.0                0.843750\n",
       "doubtfully                   135.0                0.843750\n",
       "impatiently                  135.0                0.843750\n",
       "arabian                      135.0                0.843750\n",
       "blotted                      134.5                0.840625\n",
       "errand                       133.0                0.831250\n",
       "every-day                    133.0                0.831250\n",
       "haste                        133.0                0.831250\n",
       "waking                       132.0                0.825000\n",
       "drying                       132.0                0.825000\n",
       "cellar                       132.0                0.825000\n",
       "laugh                        132.0                0.825000\n",
       "...                            ...                     ...\n",
       "lifted                       127.0                0.793750\n",
       "unused                       127.0                0.793750\n",
       "concluded                    127.0                0.793750\n",
       "tone                         127.0                0.793750\n",
       "sew                          127.0                0.793750\n",
       "folds                        127.0                0.793750\n",
       "grave                        127.0                0.793750\n",
       "eagerness                    127.0                0.793750\n",
       "sunday                       127.0                0.793750\n",
       "milder                       127.0                0.793750\n",
       "wonder                       127.0                0.793750\n",
       "wishing                      126.5                0.790625\n",
       "disappointed                 126.0                0.787500\n",
       "knitting                     126.0                0.787500\n",
       "slip                         126.0                0.787500\n",
       "never                        126.0                0.787500\n",
       "foreigners                   126.0                0.787500\n",
       "married                      126.0                0.787500\n",
       "fold                         126.0                0.787500\n",
       "winter                       126.0                0.787500\n",
       "ignorant                     126.0                0.787500\n",
       "banish                       126.0                0.787500\n",
       "burden                       125.5                0.784375\n",
       "pitied                       125.5                0.784375\n",
       "helped                       125.0                0.781250\n",
       "looked                       125.0                0.781250\n",
       "stupid                       125.0                0.781250\n",
       "craving                      125.0                0.781250\n",
       "blooming                     125.0                0.781250\n",
       "burnt                        125.0                0.781250\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(CORPUS_COMPARISON_FILENAME, index_col=0) # read in the CSV\n",
    "df2 = df.sort_values(\"Mann Whitney rho-value\", ascending=False)\n",
    "df2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
